{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50d6bc9",
   "metadata": {},
   "source": [
    "## EchoVision: An Intelligent Image Narration System for Accessibility Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a359d",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    " \n",
    "In real-world applications such as assistive technology for visually impaired individuals, smart surveillance systems, and automated content generation tools, systems must understand visual data, convert it into meaningful text, and generate natural speech output.\n",
    "This project builds a multimodal AI system integrating: \n",
    "Computer Vision (Object Detection) \n",
    "Natural Language Generation (Text Conversion) \n",
    "Speech Synthesis (Text-to-Speech) \n",
    "In this task you are supposed to **implement both the Approaches** mentioned below: \n",
    "**`<u>Approach 1 (Using Only HF):</u>`**` Using Hugging Face Transformers pipelines for object detection and Speech Synthesis.` \n",
    "**<u>Approach 2 (Using Gemini and HF):</u>** Using Google GenAI library for image captioning and HuggingFace Transformers pipeline for Speech Synthesis. \n",
    "**`<u>Approach 1: Using Hugging Face Transformers pipelines</u>`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b166642",
   "metadata": {},
   "source": [
    "## System Workflow\n",
    "\n",
    " \n",
    "Input Image → Object Detection → Label Extraction & Counting → Text Generation → Text-to-Speech → Audio Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74cebc",
   "metadata": {},
   "source": [
    "## Step-by-Step Implementation\n",
    "\n",
    "### Step 1: Object Detection from Image\n",
    " \n",
    "Objective: Detect objects present in an image and extract labels and confidence scores. \n",
    "Recommended Models: \n",
    "facebook/detr-resnet-50 (Accurate Transformer-based detector) \n",
    "hustvl/yolos-tiny (Lightweight detector) \n",
    "google/owlvit-base-patch32 (Open-vocabulary detection)[\n",
    "### Step 2: Extract Labels and Convert to Text\n",
    " \n",
    "Objective: Count occurrences of detected objects and convert them into meaningful natural language text. You have to write python logic for this step.\n",
    "### Step 3: Text-to-Speech (TTS)\n",
    " \n",
    "Objective: Convert the generated descriptive text into natural speech audio. \n",
    "Recommended Models: \n",
    "suno/bark-small \n",
    "microsoft/speecht5_tts \n",
    "facebook/fastspeech2-en-ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbc661",
   "metadata": {},
   "source": [
    "## System Architecture Overview\n",
    "\n",
    " \n",
    "Input Image↓Object Detection Model (DETR/YOLOS)↓Label Extraction & Counting↓Text Generation Module↓Text-to-Speech Model↓Audio Output \n",
    "**<u>Approach 2: using Google GenAI library and HuggingFace Transformers pipeline </u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6c579",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "\n",
    " \n",
    "Input Image → Image Captioning for object detection → Generated text → Text-to-Speech → Audio Output \n",
    "**Step-by-Step Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73856bac",
   "metadata": {},
   "source": [
    "## Step 1: Image Captioning using Google GenAI SDK\n",
    "\n",
    " \n",
    "Objective: Generate a detailed and accessibility-focused caption describing the image. \n",
    "Instead of detecting isolated objects, the model: \n",
    "Understands the entire scene \n",
    "Identifies relationships between objects \n",
    "Describes actions and context \n",
    "Produces natural, human-like language \n",
    "**Recommended Models:** gemini-3-flash-preview \n",
    "While using the given gemini model, provide the following system prompt:\n",
    "\"You are a helpful AI Assistant. Given an image perform object detection and provide a text output which contains the information about the labels detected and their counts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522518fc",
   "metadata": {},
   "source": [
    "## Step 2: Text Processing (Optional Enhancement)\n",
    "\n",
    "### **<span style=\"color:#000000\">Object</span><span style=\"color:#000000\">ive: </span><span style=\"color:#000000\">Prepare the generated caption for speech synthesis.</span>**\n",
    " \n",
    "Possible enhancements: \n",
    "Remove unnecessary symbols \n",
    "Control length (brief/detailed mode) \n",
    "Adjust tone (formal/informal) \n",
    "Add introductory phrase (e.g., \"Here is what I see in the image...\")\n",
    "### Step 3: Text-to-Speech (TTS)\n",
    " \n",
    "Objective: Convert the generated descriptive text into natural speech audio. \n",
    "Recommended Models: \n",
    "suno/bark-small \n",
    "microsoft/speecht5_tts \n",
    "facebook/fastspeech2-en-ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a93a3",
   "metadata": {},
   "source": [
    "## System Architecture Overview\n",
    "\n",
    " \n",
    "Input Image↓Object Detection using Vision Model (Google GenAI)↓Generated Text↓Text-to-Speech Model↓Audio Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb71ced",
   "metadata": {},
   "source": [
    "## Example Use Case Scenario\n",
    "\n",
    " \n",
    "Scenario: Assistive AI tool for visually impaired users.\n",
    "Sample Image: A park scene containing 3 persons, 1 bicycle, and 2 dogs.\n",
    "Generated Text: The image contains 3 persons, 1 bicycle, and 2 dogs. \n",
    "Output: Audio narration of the generated sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1563f",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    " \n",
    "Understanding transformer-based object detection \n",
    "Working with Hugging Face pipelines \n",
    "Multimodal AI system integration \n",
    "Natural language generation techniques \n",
    "Text-to-speech synthesis \n",
    "End-to-end AI system development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8984076",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    " \n",
    "This project demonstrates the integration of Vision, Language, and Speech using Transformer-based pipelines. It provides hands-on experience in designing and implementing real-world multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40f4a9",
   "metadata": {},
   "source": [
    "## Submission Link\n",
    "\n",
    " \n",
    "[**<u><span style=\"color:#1155CC\">Click here</span></u>**](https://forms.gle/NxQUoZS5FeS34kRm7) to submit your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
